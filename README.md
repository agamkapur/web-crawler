# Web Crawler

A modern, asynchronous web crawler with comprehensive URL sanitisation and normalization, redirect handling, and detailed crawl reporting. This web crawler takes a base URL as input and recursively discovers all URLs within the same domain, with advanced features for robust and efficient crawling.

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd web-crawler
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

### Command Line Interface

```bash
# Basic recursive crawl
./bin/web-crawler https://example.com

# Combine options
./bin/web-crawler https://example.com --delay 1.5 --max-redirects 8 --max-concurrent 15
```

#### CLI Options:
- `--delay`: Delay between requests in seconds (default: 0.1)
- `--max-redirects`: Maximum redirects to follow per URL (default: 10)
- `--max-concurrent`: Maximum concurrent requests (default: 10)
- `--help, -h`: Show help message

Running this command will generate a folder with the timestamp as the name in `./crawler_runs`. For example,

```
crawling_runs/
└── 2025-08-27_22-44-04/
    ├── run_details.txt          # Crawl statistics and timing
    ├── all_found_urls.txt       # All discovered URLs
    ├── all_error_urls.txt       # Error URLs (empty if no errors)
    └── all_redirect_urls.txt    # Redirect URLs (empty if no redirects)
```

## Testing

### Run All Tests
```bash
python3 -m pytest test/ -v
```

### Run Specific Test Suites
```bash
# All web crawler tests
python3 -m pytest test/test_web_crawler.py -v
```

## Problem Solving Approach

Designing and implementing a modern web crawler that is resilient and scalable is both challenging and rewarding. This web crawler was developed using an incremental approach that involved robust unit testing and benchmarking at each stage. 

1. A scooter version was developed first and this simply verified the given base URL for syntatic checks and scraped only a single webpage for URLs that shared the same domain, using BS4 for parsing the HTML.
2. The next stage added more advanced verifications and security checks for the input base URL and also added the ability to recursively crawl the URLs found on the base webpage.
3. The crawler was then tested against some popular websites sepcifically used for testing web crawlers such as `https://crawler-test.com/` and `https://demo.cyotek.com/`.
4. As a result of the above, a number of different issues were identified and fixed, such as adding the ability to track visited URLs so that the crawler would not waste time parsing the same webpage and also introducing protection against redirect loops.
5. At this point it seemed appropriate to crawl bigger websites with lots of links that were ripe for testing web crawling like `https://london.craigslist.org/` and  `https://www.wikipedia.org/`. 
6. This created the need for having asynchronous network requests because the above sites had a large number of links and performing all network I/O sequentially made the crawler really, really slow.
7. Moreover, commercial websites like these also have robust rate limiting and so polite crawling with pauses between requests was introduced.
8. Finally, some housekeeping changes were made, such as - extracting utilities to their own locations, reorganising tests, modifying logging and output routing, updating .giitignore, adding PEP8 formatting.


## Resources and Referemces Used for Development

1. [Designing a Web Crawler - HelloInterview](https://www.hellointerview.com/learn/system-design/problem-breakdowns/web-crawler)
2. [Design A Web Crawler - ByteByteGo](http://bytebytego.com/courses/system-design-interview/design-a-web-crawler)
3. [Heritrix, the web crawler for the Internet Archive](https://github.com/internetarchive/heritrix3)
4. [Crawling a billion web pages in just over 24 hours, in 2025](https://andrewkchan.dev/posts/crawler.html)

### Benchmarking and Test Websites

1. `https://crawler-test.com/`
2. `https://demo.cyotek.com/`
3. `https://london.craigslist.org/`
4. `https://www.wikipedia.org/`


## Notes on Using AI Tools for Development

1. AI tools are very useful in the planning and literature review phase of the project. Always start with asking about best resources for learning about the problem and for knowing best practices in that specific domain. Best AI Tools for this stage: ChatGPT.
2. AI builds upon and mimics patterns it encounters in the repo like a crystal growing from another crystal. Make sure to always start with coding best practices from the very beginning so that the changesets generated by AI adhere to these best practices. Best AI Tools for this stage: Cursor.
3. Always have tests from the very beginning and continue adding them incrementally because adding tests all at once later on creates spaghetti.
4. From time to time, manually read the code from top-to-bottom because in many cases AI makes some very basic mistakes because the prompt was not specific enough. No prompt can be 100% specific.
5. Use AI for recommendations first and then generate exact prompts for feature development.


## Possible Future Extensions

1. Ability to stop and restart crawler from where it left off, higher degree of fault tolerance
2. Using dedicated persisted databases for building a frontier queue of URLs
3. Distributed key-value store for checking visited URLs when multiple instances of the crawler are running


## Features

### Core Functionality
- **Asynchronous Crawling**: High-performance concurrent request handling
- **URL Normalization**: Comprehensive URL standardization to prevent duplicate crawling
- **Redirect Loop Detection**: Advanced protection against infinite, reverse, and circular redirect loops
- **Crawl Reports**: Automatic generation of detailed crawl reports with timestamps
- **Domain Restriction**: Only crawls URLs from the same domain as the base URL
- **Error Tracking**: Comprehensive tracking of error URLs and redirect URLs
- **Unlimited Depth**: Recursively discovers all URLs without depth limitations
- **Polite Crawling**: Configurable delays between requests to be respectful to servers

## Project Structure

```
web-crawler/
├── bin/
│   └── web-crawler              # CLI executable
├── src/
│   ├── web_crawler.py           # Main crawling logic
│   └── utils/
│       ├── url_normalizer.py    # URL normalization
│       ├── redirect_handler.py  # Redirect handling
│       └── verification_utils.py # URL verification
├── test/
│   ├── test_web_crawler.py      # Web crawler tests
│   └── utils/
│       └── test_verification_utils.py # Verification utils tests
├── crawling_runs/               # Generated crawl reports
│   └── YYYY-MM-DD_HH-MM-SS/     # Timestamped report folders
├── requirements.txt             # Python dependencies
└── README.md                    # This file
```

