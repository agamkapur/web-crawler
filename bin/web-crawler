#!/usr/bin/env python3
"""
Web Crawler

A command line tool that recursively crawls a website starting from a base URL and prints all discovered URLs.
It will only crawl URLs within the same domain as the base URL and maintains a list of visited URLs to avoid duplicates.
"""

import os
import sys
import argparse

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))

from web_crawler import crawl


def main():
    parser = argparse.ArgumentParser(
        description="Recursively crawl a website and extract all URLs within the same domain",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  ./bin/web-crawler https://example.com                    # Recursive crawl (unlimited depth)
  ./bin/web-crawler https://example.com --depth 2          # Crawl up to depth 2
  ./bin/web-crawler https://example.com --delay 0.5        # 0.5 second delay between requests
  ./bin/web-crawler https://example.com --max-redirects 5   # Maximum 5 redirects per URL
  ./bin/web-crawler https://example.com --max-concurrent 20 # Maximum 20 concurrent requests
        """
    )
    
    parser.add_argument("base_url", help="The base URL to start crawling from")
    parser.add_argument("--depth", "-d", type=int, help="Maximum depth for recursive crawling (default: unlimited)")
    parser.add_argument("--delay", type=float, default=0.1, help="Delay between requests in seconds (default: 0.1)")
    parser.add_argument("--max-redirects", type=int, default=10, help="Maximum redirects to follow per URL (default: 10)")
    parser.add_argument("--max-concurrent", type=int, default=10, help="Maximum concurrent requests (default: 10)")
    
    args = parser.parse_args()
    
    try:
        crawl(args.base_url, max_depth=args.depth, delay=args.delay, 
              max_redirects=args.max_redirects, max_concurrent=args.max_concurrent)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()